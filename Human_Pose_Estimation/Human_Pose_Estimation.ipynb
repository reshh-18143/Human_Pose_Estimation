{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxnlhm91S8yb",
        "outputId": "23ecc6ca-b34a-440d-9856-600e3278ca11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UbOKhA1xkN-",
        "outputId": "36ed9a61-c9d1-4a78-ee45-163988aa71e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "!pip install --upgrade matplotlib\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "import urllib.request\n",
        "from IPython.display import HTML, display\n",
        "import imageio\n",
        "import os\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KEYPOINT_DICT = {\n",
        "    'nose': 0, 'left_eye': 1, 'right_eye': 2, 'left_ear': 3, 'right_ear': 4,\n",
        "    'left_shoulder': 5, 'right_shoulder': 6, 'left_elbow': 7, 'right_elbow': 8,\n",
        "    'left_wrist': 9, 'right_wrist': 10, 'left_hip': 11, 'right_hip': 12,\n",
        "    'left_knee': 13, 'right_knee': 14, 'left_ankle': 15, 'right_ankle': 16\n",
        "}\n",
        "\n",
        "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
        "    (0, 1): 'm', (0, 2): 'c', (1, 3): 'm', (2, 4): 'c', (0, 5): 'm', (0, 6): 'c',\n",
        "    (5, 7): 'm', (7, 9): 'm', (6, 8): 'c', (8, 10): 'c', (5, 6): 'y', (5, 11): 'm',\n",
        "    (6, 12): 'c', (11, 12): 'y', (11, 13): 'm', (13, 15): 'm', (12, 14): 'c', (14, 16): 'c'\n",
        "}\n",
        "\n",
        "def get_keypoints_and_edges(keypoints_with_scores, height, width, keypoint_threshold=0.11):\n",
        "    keypoints_all, keypoint_edges_all, edge_colors = [], [], []\n",
        "\n",
        "    num_instances, _, _, _ = keypoints_with_scores.shape\n",
        "\n",
        "    for idx in range(num_instances):\n",
        "        kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
        "        kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
        "        kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
        "\n",
        "        kpts_absolute_xy = np.stack([width * kpts_x, height * kpts_y], axis=-1)\n",
        "        keypoints_above_thresh = kpts_absolute_xy[kpts_scores > keypoint_threshold]\n",
        "        keypoints_all.append(keypoints_above_thresh)\n",
        "\n",
        "        for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
        "            if kpts_scores[edge_pair[0]] > keypoint_threshold and kpts_scores[edge_pair[1]] > keypoint_threshold:\n",
        "                keypoint_edges_all.append(\n",
        "                    np.array([\n",
        "                        [kpts_absolute_xy[edge_pair[0], 0], kpts_absolute_xy[edge_pair[0], 1]],\n",
        "                        [kpts_absolute_xy[edge_pair[1], 0], kpts_absolute_xy[edge_pair[1], 1]]\n",
        "                    ])\n",
        "                )\n",
        "                edge_colors.append(color)\n",
        "\n",
        "    keypoints_xy = np.concatenate(keypoints_all, axis=0) if keypoints_all else np.zeros((0, 2))\n",
        "    edges_xy = np.stack(keypoint_edges_all, axis=0) if keypoint_edges_all else np.zeros((0, 2, 2))\n",
        "\n",
        "    return keypoints_xy, edges_xy, edge_colors\n",
        "\n",
        "def draw_pose(image, keypoints_with_scores):\n",
        "    height, width, _ = image.shape\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ax.imshow(image)\n",
        "    line_segments = LineCollection([], linewidths=4)\n",
        "    ax.add_collection(line_segments)\n",
        "\n",
        "    keypoint_locs, keypoint_edges, edge_colors = get_keypoints_and_edges(keypoints_with_scores, height, width)\n",
        "\n",
        "    if keypoint_edges.size > 0:\n",
        "        line_segments.set_segments(keypoint_edges)\n",
        "        line_segments.set_color(edge_colors)\n",
        "\n",
        "    if keypoint_locs.size > 0:\n",
        "        ax.scatter(*zip(*keypoint_locs), s=60, color='#FF1493', zorder=3)\n",
        "\n",
        "    output_path = \"output_pose.png\"\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    print(f\"Pose output saved at {output_path}\")\n",
        "    return imageio.imread(output_path)"
      ],
      "metadata": {
        "id": "vcxFnRgxjfaF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
        "try:\n",
        "    movenet_model = hub.load(model_url)\n",
        "    print(\"MoveNet model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    raise\n",
        "\n",
        "def movenet(input_image):\n",
        "    model = movenet_model.signatures['serving_default']\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    outputs = model(input_image)\n",
        "    return outputs['output_0'].numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7NG2Iz-jfWk",
        "outputId": "b2168da5-45ce-4acf-cc21-d4a107eac414"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoveNet model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = \"https://images.pexels.com/photos/4384679/pexels-photo-4384679.jpeg\"\n",
        "image_path = \"input_image.jpeg\"\n",
        "\n",
        "print(\"Downloading image...\")\n",
        "try:\n",
        "    req = urllib.request.Request(image_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    with urllib.request.urlopen(req) as response, open(image_path, 'wb') as f:\n",
        "        f.write(response.read())\n",
        "    print(\"Image downloaded successfully.\")\n",
        "except urllib.error.HTTPError as e:\n",
        "    print(f\"Failed to download image: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SIOf-KqjfUE",
        "outputId": "bde92560-660c-41ab-f3dd-b69f10814be0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading image...\n",
            "Image downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_jpeg(image)\n",
        "input_size = 192\n",
        "input_image = tf.image.resize_with_pad(image, input_size, input_size)\n",
        "input_image = tf.expand_dims(input_image, axis=0)\n",
        "\n",
        "keypoints_with_scores = movenet(input_image)\n",
        "\n",
        "output_image = draw_pose(image.numpy(), keypoints_with_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAWJdmzijfRa",
        "outputId": "02a748ab-a3ba-4d62-818d-666257e63654"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pose output saved at output_pose.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-673ab90255d7>:67: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  return imageio.imread(output_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "MAF7mXDe_Td-"
      },
      "outputs": [],
      "source": [
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "    \"\"\"Defines the initial crop region.\"\"\"\n",
        "    if image_width > image_height:\n",
        "        box_height = image_width / image_height\n",
        "        box_width = 1.0\n",
        "        y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "        x_min = 0.0\n",
        "    else:\n",
        "        box_height = 1.0\n",
        "        box_width = image_height / image_width\n",
        "        y_min = 0.0\n",
        "        x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "    return {\n",
        "        'y_min': y_min,\n",
        "        'x_min': x_min,\n",
        "        'y_max': y_min + box_height,\n",
        "        'x_max': x_min + box_width,\n",
        "        'height': box_height,\n",
        "        'width': box_width\n",
        "    }\n",
        "\n",
        "def torso_visible(keypoints):\n",
        "    \"\"\"Checks whether there are enough torso keypoints.\"\"\"\n",
        "    return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "             MIN_CROP_KEYPOINT_SCORE or\n",
        "             keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "             MIN_CROP_KEYPOINT_SCORE) and\n",
        "            (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "             MIN_CROP_KEYPOINT_SCORE or\n",
        "             keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "             MIN_CROP_KEYPOINT_SCORE))\n",
        "\n",
        "\n",
        "def determine_torso_and_body_range(\n",
        "        keypoints, target_keypoints, center_y, center_x):\n",
        "    \"\"\"Calculates the maximum range of torso and body keypoints.\"\"\"\n",
        "    torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "    max_torso_yrange = 0.0\n",
        "    max_torso_xrange = 0.0\n",
        "    max_body_yrange = 0.0\n",
        "    max_body_xrange = 0.0\n",
        "    for joint in torso_joints:\n",
        "        dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "        dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "        if dist_y > max_torso_yrange:\n",
        "            max_torso_yrange = dist_y\n",
        "        if dist_x > max_torso_xrange:\n",
        "            max_torso_xrange = dist_x\n",
        "\n",
        "    for joint in KEYPOINT_DICT.keys():\n",
        "        if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "            continue\n",
        "        dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "        dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "        if dist_y > max_body_yrange:\n",
        "            max_body_yrange = dist_y\n",
        "\n",
        "        if dist_x > max_body_xrange:\n",
        "            max_body_xrange = dist_x\n",
        "\n",
        "    return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
        "\n",
        "\n",
        "def determine_crop_region(\n",
        "        keypoints, image_height, image_width):\n",
        "    \"\"\"Determines the region to crop the image.\"\"\"\n",
        "    target_keypoints = {}\n",
        "    for joint in KEYPOINT_DICT.keys():\n",
        "        target_keypoints[joint] = [\n",
        "            keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "            keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "        ]\n",
        "    if torso_visible(keypoints):\n",
        "        center_y = (target_keypoints['left_hip'][0] +\n",
        "                    target_keypoints['right_hip'][0]) / 2\n",
        "        center_x = (target_keypoints['left_hip'][1] +\n",
        "                    target_keypoints['right_hip'][1]) / 2\n",
        "\n",
        "        (max_torso_yrange, max_torso_xrange,\n",
        "         max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "            keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "        crop_length_half = np.amax(\n",
        "            [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "             max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "        tmp = np.array(\n",
        "            [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "        crop_length_half = np.amin(\n",
        "            [crop_length_half, np.amax(tmp)])\n",
        "\n",
        "        crop_corner = [center_y - crop_length_half, center_x - crop_length_half]\n",
        "\n",
        "        if crop_length_half > max(image_width, image_height) / 2:\n",
        "            return init_crop_region(image_height, image_width)\n",
        "        else:\n",
        "            crop_length = crop_length_half * 2\n",
        "            return {\n",
        "                'y_min': crop_corner[0] / image_height,\n",
        "                'x_min': crop_corner[1] / image_width,\n",
        "                'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "                'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "                'height': (crop_corner[0] + crop_length) / image_height - crop_corner[0] / image_height,\n",
        "                'width': (crop_corner[1] + crop_length) / image_width - crop_corner[1] / image_width\n",
        "            }\n",
        "    else:\n",
        "        return init_crop_region(image_height, image_width)\n",
        "\n",
        "\n",
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "    \"\"\"Crops and resizes the image based on the crop region.\"\"\"\n",
        "    boxes = [[crop_region['y_min'], crop_region['x_min'],\n",
        "              crop_region['y_max'], crop_region['x_max']]]\n",
        "    output_image = tf.image.crop_and_resize(\n",
        "        image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "    return output_image\n",
        "\n",
        "\n",
        "def run_interface(movenet, image, crop_region, crop_size):\n",
        "    \"\"\"Runs the model interface to estimate keypoints.\"\"\"\n",
        "    image_height, image_width, _ = image.shape\n",
        "    input_image = crop_and_resize(\n",
        "        tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "    keypoints_with_scores = movenet(input_image)\n",
        "    for idx in range(17):\n",
        "        keypoints_with_scores[0, 0, idx, 0] = (\n",
        "                crop_region['y_min'] * image_height +\n",
        "                crop_region['height'] * image_height * keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "        keypoints_with_scores[0, 0, idx, 1] = (\n",
        "                crop_region['x_min'] * image_width +\n",
        "                crop_region['width'] * image_width * keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "\n",
        "    return keypoints_with_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HgRKktYig06P"
      },
      "outputs": [],
      "source": [
        "!wget -q -O dance.gif https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/dance_input.gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeaJmFuUgf-B",
        "outputId": "86f3b73c-66bd-4ce2-f8b1-081e6488daad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing frames:   0%|          | 0/42 [00:00<?, ?it/s]<ipython-input-25-96975e021d89>:65: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  output_image = imageio.imread(\"output_image.png\")\n",
            "Processing frames:   2%|▏         | 1/42 [00:03<02:38,  3.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:   5%|▍         | 2/42 [00:04<01:25,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:   7%|▋         | 3/42 [00:05<00:59,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  10%|▉         | 4/42 [00:06<00:52,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  12%|█▏        | 5/42 [00:07<00:46,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  14%|█▍        | 6/42 [00:08<00:44,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  17%|█▋        | 7/42 [00:10<00:46,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  19%|█▉        | 8/42 [00:11<00:46,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  21%|██▏       | 9/42 [00:12<00:37,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  24%|██▍       | 10/42 [00:13<00:33,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  26%|██▌       | 11/42 [00:14<00:29,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  29%|██▊       | 12/42 [00:15<00:30,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  31%|███       | 13/42 [00:16<00:27,  1.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  33%|███▎      | 14/42 [00:17<00:26,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  36%|███▌      | 15/42 [00:17<00:23,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  38%|███▊      | 16/42 [00:18<00:22,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  40%|████      | 17/42 [00:19<00:18,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  43%|████▎     | 18/42 [00:19<00:15,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  45%|████▌     | 19/42 [00:19<00:13,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  48%|████▊     | 20/42 [00:20<00:12,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  50%|█████     | 21/42 [00:20<00:11,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  52%|█████▏    | 22/42 [00:21<00:10,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  55%|█████▍    | 23/42 [00:21<00:09,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  57%|█████▋    | 24/42 [00:22<00:10,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  60%|█████▉    | 25/42 [00:23<00:09,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  62%|██████▏   | 26/42 [00:23<00:09,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  64%|██████▍   | 27/42 [00:24<00:09,  1.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  67%|██████▋   | 28/42 [00:25<00:08,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  69%|██████▉   | 29/42 [00:25<00:07,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  71%|███████▏  | 30/42 [00:25<00:06,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  74%|███████▍  | 31/42 [00:26<00:05,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  76%|███████▌  | 32/42 [00:26<00:04,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  79%|███████▊  | 33/42 [00:27<00:04,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  81%|████████  | 34/42 [00:27<00:03,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  83%|████████▎ | 35/42 [00:28<00:03,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  86%|████████▌ | 36/42 [00:28<00:02,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  88%|████████▊ | 37/42 [00:29<00:02,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  90%|█████████ | 38/42 [00:29<00:01,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  93%|█████████▎| 39/42 [00:30<00:01,  2.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  95%|█████████▌| 40/42 [00:30<00:00,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing frames:  98%|█████████▊| 41/42 [00:31<00:00,  2.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing frames: 100%|██████████| 42/42 [00:31<00:00,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved as 'output_image.png'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "image_path = 'dance.gif'\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_gif(image)\n",
        "\n",
        "num_frames, image_height, image_width, _ = image.shape\n",
        "\n",
        "crop_region = init_crop_region(image_height, image_width)\n",
        "output_images = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "for frame_idx in tqdm(range(num_frames), total=num_frames, desc=\"Processing frames\"):\n",
        "    keypoints_with_scores = run_interface(\n",
        "        movenet, image[frame_idx, :, :, :], crop_region,\n",
        "        crop_size=[input_size, input_size])\n",
        "    output_images.append(draw_prediction_on_image(\n",
        "        image[frame_idx, :, :, :].numpy().astype(np.int32),\n",
        "        keypoints_with_scores, crop_region=None,\n",
        "        close_figure=True, output_image_height=300))\n",
        "    crop_region = determine_crop_region(\n",
        "        keypoints_with_scores, image_height, image_width)\n",
        "\n",
        "output = np.stack(output_images, axis=0)\n",
        "imageio.mimsave('output.gif', output, fps=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}