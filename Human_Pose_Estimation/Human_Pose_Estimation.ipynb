{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxnlhm91S8yb",
        "outputId": "3ecf310b-a6c4-48ef-a393-deeaea267395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UbOKhA1xkN-",
        "outputId": "f3d416bb-ace7-49b0-ca54-725bcaade6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "Successfully installed matplotlib-3.10.1\n"
          ]
        }
      ],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "!pip install --upgrade matplotlib\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "import urllib.request\n",
        "from IPython.display import HTML, display\n",
        "import imageio\n",
        "import os\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KEYPOINT_DICT = {\n",
        "    'nose': 0, 'left_eye': 1, 'right_eye': 2, 'left_ear': 3, 'right_ear': 4,\n",
        "    'left_shoulder': 5, 'right_shoulder': 6, 'left_elbow': 7, 'right_elbow': 8,\n",
        "    'left_wrist': 9, 'right_wrist': 10, 'left_hip': 11, 'right_hip': 12,\n",
        "    'left_knee': 13, 'right_knee': 14, 'left_ankle': 15, 'right_ankle': 16\n",
        "}\n",
        "\n",
        "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
        "    (0, 1): 'm', (0, 2): 'c', (1, 3): 'm', (2, 4): 'c', (0, 5): 'm', (0, 6): 'c',\n",
        "    (5, 7): 'm', (7, 9): 'm', (6, 8): 'c', (8, 10): 'c', (5, 6): 'y', (5, 11): 'm',\n",
        "    (6, 12): 'c', (11, 12): 'y', (11, 13): 'm', (13, 15): 'm', (12, 14): 'c', (14, 16): 'c'\n",
        "}\n",
        "\n",
        "def draw_prediction_on_image(\n",
        "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
        "    output_image_height=None):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        "\n",
        "  Args:\n",
        "    image: A numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
        "      of the crop region in normalized coordinates (see the init_crop_region\n",
        "      function below for more detail). If provided, this function will also\n",
        "      draw the bounding box on the image.\n",
        "    output_image_height: An integer indicating the height of the output image.\n",
        "      Note that the image aspect ratio will be the same as the input image.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  # To remove the huge white borders\n",
        "  fig.tight_layout(pad=0)\n",
        "  ax.margins(0)\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_xticklabels([])\n",
        "  plt.axis('off')\n",
        "\n",
        "  im = ax.imshow(image)\n",
        "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
        "  ax.add_collection(line_segments)\n",
        "  # Turn off tick labels\n",
        "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
        "\n",
        "  (keypoint_locs, keypoint_edges,\n",
        "   edge_colors) = get_keypoints_and_edges(keypoints_with_scores, height, width)\n",
        "\n",
        "  line_segments.set_segments(keypoint_edges)\n",
        "  line_segments.set_color(edge_colors)\n",
        "  if keypoint_edges.shape[0]:\n",
        "    line_segments.set_segments(keypoint_edges)\n",
        "    line_segments.set_color(edge_colors)\n",
        "  if keypoint_locs.shape[0]:\n",
        "    scat.set_offsets(keypoint_locs)\n",
        "\n",
        "  if crop_region is not None:\n",
        "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
        "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
        "    rec_width = min(crop_region['x_max'], 1.0) * width - xmin\n",
        "    rec_height = min(crop_region['y_max'], 1.0) * height - ymin\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin,ymin),rec_width,rec_height,\n",
        "        linewidth=1,edgecolor='b',facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  # Use fig.canvas.buffer_rgba() and np.frombuffer() with a suitable offset and stride\n",
        "  image_from_plot = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
        "  image_from_plot = image_from_plot.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
        "  image_from_plot = image_from_plot[:, :, :3] # Remove the alpha channel\n",
        "\n",
        "  plt.close(fig)\n",
        "  if output_image_height is not None:\n",
        "    output_image_width = int(output_image_height / height * width)\n",
        "    image_from_plot = cv2.resize(\n",
        "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
        "         interpolation=cv2.INTER_CUBIC)\n",
        "  return image_from_plot\n",
        "\n",
        "def get_keypoints_and_edges(keypoints_with_scores, height, width, keypoint_threshold=0.11):\n",
        "    keypoints_all, keypoint_edges_all, edge_colors = [], [], []\n",
        "\n",
        "    num_instances, _, _, _ = keypoints_with_scores.shape\n",
        "\n",
        "    for idx in range(num_instances):\n",
        "        kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
        "        kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
        "        kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
        "\n",
        "        kpts_absolute_xy = np.stack([width * kpts_x, height * kpts_y], axis=-1)\n",
        "        keypoints_above_thresh = kpts_absolute_xy[kpts_scores > keypoint_threshold]\n",
        "        keypoints_all.append(keypoints_above_thresh)\n",
        "\n",
        "        for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
        "            if kpts_scores[edge_pair[0]] > keypoint_threshold and kpts_scores[edge_pair[1]] > keypoint_threshold:\n",
        "                keypoint_edges_all.append(\n",
        "                    np.array([\n",
        "                        [kpts_absolute_xy[edge_pair[0], 0], kpts_absolute_xy[edge_pair[0], 1]],\n",
        "                        [kpts_absolute_xy[edge_pair[1], 0], kpts_absolute_xy[edge_pair[1], 1]]\n",
        "                    ])\n",
        "                )\n",
        "                edge_colors.append(color)\n",
        "\n",
        "    keypoints_xy = np.concatenate(keypoints_all, axis=0) if keypoints_all else np.zeros((0, 2))\n",
        "    edges_xy = np.stack(keypoint_edges_all, axis=0) if keypoint_edges_all else np.zeros((0, 2, 2))\n",
        "\n",
        "    return keypoints_xy, edges_xy, edge_colors\n",
        "\n",
        "def draw_pose(image, keypoints_with_scores):\n",
        "    height, width, _ = image.shape\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ax.imshow(image)\n",
        "    line_segments = LineCollection([], linewidths=4)\n",
        "    ax.add_collection(line_segments)\n",
        "\n",
        "    keypoint_locs, keypoint_edges, edge_colors = get_keypoints_and_edges(keypoints_with_scores, height, width)\n",
        "\n",
        "    if keypoint_edges.size > 0:\n",
        "        line_segments.set_segments(keypoint_edges)\n",
        "        line_segments.set_color(edge_colors)\n",
        "\n",
        "    if keypoint_locs.size > 0:\n",
        "        ax.scatter(*zip(*keypoint_locs), s=60, color='#FF1493', zorder=3)\n",
        "\n",
        "    output_path = \"output_pose.png\"\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    print(f\"Pose output saved at {output_path}\")\n",
        "    return imageio.imread(output_path)"
      ],
      "metadata": {
        "id": "vcxFnRgxjfaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
        "try:\n",
        "    movenet_model = hub.load(model_url)\n",
        "    print(\"MoveNet model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    raise\n",
        "\n",
        "def movenet(input_image):\n",
        "    model = movenet_model.signatures['serving_default']\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    outputs = model(input_image)\n",
        "    return outputs['output_0'].numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7NG2Iz-jfWk",
        "outputId": "6436686f-f955-46a4-ea23-97f368b8f3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoveNet model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = \"https://images.pexels.com/photos/4384679/pexels-photo-4384679.jpeg\"\n",
        "image_path = \"input_image.jpeg\"\n",
        "\n",
        "print(\"Downloading image...\")\n",
        "try:\n",
        "    req = urllib.request.Request(image_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    with urllib.request.urlopen(req) as response, open(image_path, 'wb') as f:\n",
        "        f.write(response.read())\n",
        "    print(\"Image downloaded successfully.\")\n",
        "except urllib.error.HTTPError as e:\n",
        "    print(f\"Failed to download image: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SIOf-KqjfUE",
        "outputId": "9c7d59fd-a5b9-4545-c398-bbcbf262d48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading image...\n",
            "Image downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_jpeg(image)\n",
        "input_size = 192\n",
        "input_image = tf.image.resize_with_pad(image, input_size, input_size)\n",
        "input_image = tf.expand_dims(input_image, axis=0)\n",
        "\n",
        "keypoints_with_scores = movenet(input_image)\n",
        "\n",
        "output_image = draw_pose(image.numpy(), keypoints_with_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAWJdmzijfRa",
        "outputId": "ab9b7762-bd76-46b7-ed35-3024e2bf3cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pose output saved at output_pose.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-394e165ca78a>:65: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  return imageio.imread(output_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAF7mXDe_Td-"
      },
      "outputs": [],
      "source": [
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "    \"\"\"Defines the initial crop region.\"\"\"\n",
        "    if image_width > image_height:\n",
        "        box_height = image_width / image_height\n",
        "        box_width = 1.0\n",
        "        y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "        x_min = 0.0\n",
        "    else:\n",
        "        box_height = 1.0\n",
        "        box_width = image_height / image_width\n",
        "        y_min = 0.0\n",
        "        x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "    return {\n",
        "        'y_min': y_min,\n",
        "        'x_min': x_min,\n",
        "        'y_max': y_min + box_height,\n",
        "        'x_max': x_min + box_width,\n",
        "        'height': box_height,\n",
        "        'width': box_width\n",
        "    }\n",
        "\n",
        "def torso_visible(keypoints):\n",
        "    \"\"\"Checks whether there are enough torso keypoints.\"\"\"\n",
        "    return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "             MIN_CROP_KEYPOINT_SCORE or\n",
        "             keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "             MIN_CROP_KEYPOINT_SCORE) and\n",
        "            (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "             MIN_CROP_KEYPOINT_SCORE or\n",
        "             keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "             MIN_CROP_KEYPOINT_SCORE))\n",
        "\n",
        "\n",
        "def determine_torso_and_body_range(\n",
        "        keypoints, target_keypoints, center_y, center_x):\n",
        "    \"\"\"Calculates the maximum range of torso and body keypoints.\"\"\"\n",
        "    torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "    max_torso_yrange = 0.0\n",
        "    max_torso_xrange = 0.0\n",
        "    max_body_yrange = 0.0\n",
        "    max_body_xrange = 0.0\n",
        "    for joint in torso_joints:\n",
        "        dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "        dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "        if dist_y > max_torso_yrange:\n",
        "            max_torso_yrange = dist_y\n",
        "        if dist_x > max_torso_xrange:\n",
        "            max_torso_xrange = dist_x\n",
        "\n",
        "    for joint in KEYPOINT_DICT.keys():\n",
        "        if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "            continue\n",
        "        dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "        dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "        if dist_y > max_body_yrange:\n",
        "            max_body_yrange = dist_y\n",
        "\n",
        "        if dist_x > max_body_xrange:\n",
        "            max_body_xrange = dist_x\n",
        "\n",
        "    return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
        "\n",
        "\n",
        "def determine_crop_region(\n",
        "        keypoints, image_height, image_width):\n",
        "    \"\"\"Determines the region to crop the image.\"\"\"\n",
        "    target_keypoints = {}\n",
        "    for joint in KEYPOINT_DICT.keys():\n",
        "        target_keypoints[joint] = [\n",
        "            keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "            keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "        ]\n",
        "    if torso_visible(keypoints):\n",
        "        center_y = (target_keypoints['left_hip'][0] +\n",
        "                    target_keypoints['right_hip'][0]) / 2\n",
        "        center_x = (target_keypoints['left_hip'][1] +\n",
        "                    target_keypoints['right_hip'][1]) / 2\n",
        "\n",
        "        (max_torso_yrange, max_torso_xrange,\n",
        "         max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "            keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "        crop_length_half = np.amax(\n",
        "            [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "             max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "        tmp = np.array(\n",
        "            [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "        crop_length_half = np.amin(\n",
        "            [crop_length_half, np.amax(tmp)])\n",
        "\n",
        "        crop_corner = [center_y - crop_length_half, center_x - crop_length_half]\n",
        "\n",
        "        if crop_length_half > max(image_width, image_height) / 2:\n",
        "            return init_crop_region(image_height, image_width)\n",
        "        else:\n",
        "            crop_length = crop_length_half * 2\n",
        "            return {\n",
        "                'y_min': crop_corner[0] / image_height,\n",
        "                'x_min': crop_corner[1] / image_width,\n",
        "                'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "                'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "                'height': (crop_corner[0] + crop_length) / image_height - crop_corner[0] / image_height,\n",
        "                'width': (crop_corner[1] + crop_length) / image_width - crop_corner[1] / image_width\n",
        "            }\n",
        "    else:\n",
        "        return init_crop_region(image_height, image_width)\n",
        "\n",
        "\n",
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "    \"\"\"Crops and resizes the image based on the crop region.\"\"\"\n",
        "    boxes = [[crop_region['y_min'], crop_region['x_min'],\n",
        "              crop_region['y_max'], crop_region['x_max']]]\n",
        "    output_image = tf.image.crop_and_resize(\n",
        "        image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "    return output_image\n",
        "\n",
        "\n",
        "def run_interface(movenet, image, crop_region, crop_size):\n",
        "    \"\"\"Runs the model interface to estimate keypoints.\"\"\"\n",
        "    image_height, image_width, _ = image.shape\n",
        "    input_image = crop_and_resize(\n",
        "        tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "    keypoints_with_scores = movenet(input_image)\n",
        "    for idx in range(17):\n",
        "        keypoints_with_scores[0, 0, idx, 0] = (\n",
        "                crop_region['y_min'] * image_height +\n",
        "                crop_region['height'] * image_height * keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "        keypoints_with_scores[0, 0, idx, 1] = (\n",
        "                crop_region['x_min'] * image_width +\n",
        "                crop_region['width'] * image_width * keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "\n",
        "    return keypoints_with_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgRKktYig06P"
      },
      "outputs": [],
      "source": [
        "!wget -q -O dance.gif https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/dance_input.gif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeaJmFuUgf-B",
        "outputId": "bc4311bd-e918-41bd-f2d2-f807480938f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing frames: 100%|██████████| 42/42 [00:13<00:00,  3.05it/s]\n"
          ]
        }
      ],
      "source": [
        "image_path = 'dance.gif'\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_gif(image)\n",
        "\n",
        "num_frames, image_height, image_width, _ = image.shape\n",
        "\n",
        "crop_region = init_crop_region(image_height, image_width)\n",
        "output_images = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "for frame_idx in tqdm(range(num_frames), total=num_frames, desc=\"Processing frames\"):\n",
        "    keypoints_with_scores = run_interface(\n",
        "        movenet, image[frame_idx, :, :, :], crop_region,\n",
        "        crop_size=[input_size, input_size])\n",
        "    output_images.append(draw_prediction_on_image(\n",
        "        image[frame_idx, :, :, :].numpy().astype(np.int32),\n",
        "        keypoints_with_scores, crop_region=None,\n",
        "        close_figure=True, output_image_height=300))\n",
        "    crop_region = determine_crop_region(\n",
        "        keypoints_with_scores, image_height, image_width)\n",
        "\n",
        "output = np.stack(output_images, axis=0)\n",
        "imageio.mimsave('output.gif', output, fps=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}